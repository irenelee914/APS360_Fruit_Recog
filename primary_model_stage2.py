# -*- coding: utf-8 -*-
"""primary_model_stage2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14azeQxkIawg9P64lNg4D1F1BUFdR6SFF
"""

#mount googledrive
from google.colab import drive
drive.mount('/content/gdrive')

import torch
import numpy as np
import time
import os
import torchvision
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import matplotlib.pyplot as plt

#import pretrained vgg16

import torchvision.models
vgg16  = torchvision.models.vgg16 (pretrained=True)

import torch.nn as nn
  import torch.optim as optim
  import torch.nn.functional as F

class TransferNN(nn.Module):
    def __init__(self):
        super(TransferNN, self).__init__()
        self.fc1 = nn.Linear(512 * 7 * 7, 40)
        self.fc2 = nn.Linear(40, 30)

    def forward(self, x):
        x = x.view(-1, 512 * 7 * 7) #flatten feature data
        x = F.relu(self.fc1(x))
        x = self.fc2(x)
        return x

TransferNet = TransferNN()
state = torch.load('/content/gdrive/My Drive/Colab Notebooks/Fruit_Project/bs32_lr0.001_epoch9_FINAL_2layer_40hidden.001_epoch9')
TransferNet.load_state_dict(state)

def get_accuracy_vgg16_modified(model, data_loader,fruit_label):
    correct = 0
    total = 0
    for imgs, labels in data_loader:
        
        if labels != fruit_label:
          continue
        if use_cuda and torch.cuda.is_available():
          imgs = imgs.cuda()
          labels = labels.cuda()

        output = model(imgs)
        pred = output.max(1, keepdim=True)[1] 
        print(pred)
        correct += pred.eq(labels.view_as(pred)).sum().item()
        total += imgs.shape[0]
    print(total)
    return correct / total

"""#data"""

import matplotlib.pyplot as plt
import matplotlib.patches as patches
from PIL import Image

# location on Google Drive of 1.png
master_path = '/content/gdrive/My Drive/Colab Notebooks/Stage2/66.png'
img = Image.open(master_path)
#show image
fig, ax = plt.subplots(1)
ax.imshow(img)

import cv2
from google.colab.patches import cv2_imshow

img = cv2.imread(master_path)
gray=cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)

th, threshed = cv2.threshold(gray, 100, 255, cv2.THRESH_BINARY_INV|cv2.THRESH_OTSU)

(cnts, _) = cv2.findContours(threshed, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)
i=0
cnts = sorted(cnts, key=cv2.contourArea)
for cnt in cnts:
    if cv2.contourArea(cnt) > 10000:
      
      # cv2.crop(img.rows,img.cols,CV_8UC3)
      # crop.setTo(scalar(0,255,0))
      result = img.copy()
      mask = np.zeros(img.shape[:2],np.uint8)
      

      result[mask==0] = (255,255,255)
      mask = 255-mask

      cv2.drawContours(mask, [cnt],-1, 255, -1)
      dst = cv2.bitwise_and(img, img, mask=mask)
      i+=1
      rect = cv2.boundingRect(cnt)

      ext_left = tuple(cnt[cnt[:, :, 0].argmin()][0])
      ext_right = tuple(cnt[cnt[:, :, 0].argmax()][0])
      ext_top = tuple(cnt[cnt[:, :, 1].argmin()][0])
      ext_bot = tuple(cnt[cnt[:, :, 1].argmax()][0])

      cv2_imshow(dst[ext_top[1]:ext_bot[1], ext_left[0]:ext_right[0]])
      cv2.imwrite("/content/gdrive/My Drive/Colab Notebooks/Stage2/Test/Apple_Crimson_Snow/"+"contour"+str(i)+'.jpg', dst[ext_top[1]:ext_bot[1], ext_left[0]:ext_right[0]])

import torch.utils.data

classes = ['Apple_Crimson_Snow', 'Apple_Granny_Smith','Apple_Pink_Lady', 'Apple_Red_Delicious','Apricot', 'Avocado','Banana', 
          'Clementine','Cocos', 'Guava','Lemon','Limes','Lychee','Mandarine', 'Mango', 'Nectarine',
          'Orange', 'Peach','Peach_Flat', 'Pear', 'Pear_Abate','Pear_Forelle','Pear_Kaiser','Pear_Monster','Pear_Red', 'Pear_Williams',
          'Plum','Pomegranate',  'Strawberry', 'Walnut']

master_path = '/content/gdrive/My Drive/Colab Notebooks/Stage2/Test'
testFeature_path = '/content/gdrive/My Drive/Colab Notebooks/Stage2/Test_Features'

transform = transforms.Compose([transforms.Resize((224,224)), 
                                transforms.ToTensor()])

test_data = torchvision.datasets.ImageFolder(master_path, transform=transform)
data_loader_test = torch.utils.data.DataLoader(test_data, batch_size=1, 
                                           num_workers=1, shuffle=False)
n = 0
for img, label in data_loader_test:
  features = vgg16.features(img)
  features_tensor = torch.from_numpy(features.detach().numpy())

  folder_name = testFeature_path + '/' + str(classes[label])
  if not os.path.isdir(folder_name):
    os.mkdir(folder_name)
  torch.save(features_tensor.squeeze(0), folder_name + '/' + str(n) + '.tensor')
  n += 1

test_feature_dataset = torchvision.datasets.DatasetFolder(testFeature_path, loader=torch.load, extensions=('.tensor'))
test_feature_loader = torch.utils.data.DataLoader(test_feature_dataset, batch_size=1, 
                                           num_workers=1, shuffle=False)

result = []

def get_accuracy_vgg16(model, data_loader):
    correct = 0
    total = 0
    for imgs, labels in data_loader:
        

        #if use_cuda and torch.cuda.is_available():
        #  imgs = imgs.cuda()
        #  labels = labels.cuda()

        output = model(imgs)
        #select index with maximum prediction score
        #pred = F.softmax(output)

        pred = output.max(1, keepdim=True)[1] 
        print(classes[pred])
        result.append(classes[pred])
        correct += pred.eq(labels.view_as(pred)).sum().item()
        total += imgs.shape[0]
    return correct / total

use_cuda = True
test_accuracy = get_accuracy_vgg16(TransferNet, test_feature_loader)

import collections

counter=collections.Counter(result)
print(counter)